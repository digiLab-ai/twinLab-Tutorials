{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Gaussian process example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Project imports\n",
    "import twinlab as tl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "campaign_dir = os.path.join(\"resources\", \"campaigns\", \"ukaea\")\n",
    "datasets_dir = os.path.join(\"resources\", \"datasets\")\n",
    "filepath_grid = os.path.join(campaign_dir, \"grid.csv\")\n",
    "filepath_train = os.path.join(datasets_dir, \"ukaea_small.csv\")\n",
    "# filepath_eval = os.path.join(campaign_dir, \"eval.csv\")\n",
    "# filepath_eval = os.path.join(campaign_dir, \"post.csv\")\n",
    "filepath_eval = os.path.join(campaign_dir, \"test.csv\")\n",
    "\n",
    "# Campaign parameters\n",
    "campaign_id = \"ukaea\"\n",
    "dataset_id = \"ukaea\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary data for training and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(filepath_train)\n",
    "df_eval = pd.read_csv(filepath_eval)\n",
    "df_grid = pd.read_csv(filepath_grid, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the parameter dictionary that we need to give to run twinLab.  At a minimum the user must provide the `filename` of the dataset on which we want to train our model (csv format), together with the columns that we will take to be the `inputs` and `outputs` of our model, once that is trained. By default, `twinLab` will train a Gaussian process (`emulator=gaussian_process`) and use all of the data for training. This latter choice can be overridden by adding e.g., `train_test_split=100` so that only the first 100 entries in `filename` are used for training, and the remaining examples can then be used for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column headings for outputs\n",
    "y_outputs = [f\"y{i}\" for i in range(len(df_grid))]\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"inputs\": [\"E1\", \"E2\", \"E3\", \"n1\", \"n2\"],\n",
    "    \"outputs\": y_outputs,\n",
    "    \"decompose_outputs\": True,\n",
    "    \"test_train_ratio\": 0.8,\n",
    "}\n",
    "pprint(params, compact=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset can be uploaded to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.upload_dataset(filepath_train, dataset_id, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the datasets to check that the upload worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tl.list_datasets(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some useful properties of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tl.query_dataset(dataset_id, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, this step should take less than two minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.train_campaign(params, campaign_id, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the campaigns again to ensure that training has been completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tl.list_campaigns(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the campaign to check how training has been"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tl.query_campaign(campaign_id, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained emulator on `X` from the evaluation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean, df_std = tl.predict_campaign(filepath_eval, campaign_id, verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results for a few different `y` values to sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for plot\n",
    "color = \"blue\"\n",
    "alpha = 0.8\n",
    "xs = {\"E1\": r\"$E_{1}$\", \"E2\": r\"$E_{2}$\", \"E3\": r\"$E_{3}$\", \"n1\": r\"$n_{1}$\", \"n2\": r\"$n_{2}$\"}\n",
    "ys = {f\"y{i}\": fr\"$y_{{{i}}}$\" for i in [0, 100, 150]}\n",
    "\n",
    "# Plot some examples\n",
    "nrow, ncol = len(ys), len(xs)\n",
    "plt.subplots(nrow, ncol, figsize=(25, 10))\n",
    "nplot = 0\n",
    "for y, y_label in ys.items():\n",
    "    for x, x_label in xs.items():\n",
    "        nplot += 1\n",
    "        plt.subplot(nrow, ncol, nplot)\n",
    "        plt.errorbar(df_eval[x], df_mean[y], yerr=df_std[y], marker='.', lw=1, ls='None', color=color, alpha=alpha, label=\"Model\")\n",
    "        plt.plot(df_train[x], df_train[y], \".\", color=\"black\", alpha=0.1, label=\"Training data\")\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        if nplot==1: plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output of the trained function together with the \"truth\" from the evaluation file. The intensity of the blue regions here correspond to the probability. It can be seen that the truth mainly goes through the regions of high probability as predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for plot\n",
    "error_inflation_factor = 1. # Factor to multiply error by for plotting\n",
    "y_fac = 18 # Factor to divide y by for plotting [log10]\n",
    "plot_eval = True\n",
    "data_alpha = 0.75\n",
    "plot_model_mean = False\n",
    "plot_model_bands = True\n",
    "plot_model_blur = False\n",
    "nsigs = [1, 2]\n",
    "model_alpha = 0.75\n",
    "n_model_blur = 100\n",
    "model_color = \"blue\"\n",
    "number_of_training_examples = 0\n",
    "number_of_model_examples = 5\n",
    "\n",
    "# Plot results\n",
    "grid = df_grid.iloc[:, 0]\n",
    "plt.subplots()\n",
    "if (plot_model_blur or plot_model_bands) and not plot_model_mean: \n",
    "    plt.fill_between(grid, np.nan, np.nan, color=model_color, alpha=model_alpha, lw=0., label=\"Model predictions\")\n",
    "for example in range(number_of_training_examples): # Training examples\n",
    "    train = df_train[y_outputs].iloc[example]/10**y_fac\n",
    "    label = \"Example training data\" if example==0 else None\n",
    "    plt.plot(grid, train, color=\"black\", alpha=0.5, label=label)\n",
    "for example in range(number_of_model_examples): # Model predictions\n",
    "    mean = df_mean[y_outputs].iloc[example]/10**y_fac\n",
    "    err = error_inflation_factor*df_std[y_outputs].iloc[example]/10**y_fac\n",
    "    if plot_eval and (filepath_eval == os.path.join(campaign_dir, \"test.csv\")):\n",
    "        eval = df_eval[y_outputs].iloc[example]/10**y_fac\n",
    "        label = \"Test data\" if example==0 else None\n",
    "        plt.plot(grid, eval, color=\"black\", alpha=data_alpha, label=label)\n",
    "    if plot_model_mean:\n",
    "        label = \"Model predictions\" if example==0 else None\n",
    "        plt.plot(grid, mean, color=model_color, label=label, alpha=model_alpha)\n",
    "    elif plot_model_bands:\n",
    "        for nsig in nsigs:\n",
    "            plt.fill_between(grid, mean-nsig*err, mean+nsig*err, color=model_color, alpha=model_alpha/nsig, lw=0.)\n",
    "    elif plot_model_blur:\n",
    "        alpha = tl.get_blur_alpha(n_model_blur, model_alpha)\n",
    "        dys = tl.get_blur_boundaries(n_model_blur)\n",
    "        for dy in dys:\n",
    "            plt.fill_between(grid, mean-dy*err, mean+dy*err, color=model_color, alpha=alpha, lw=0.)\n",
    "plt.xlabel(r'Temperature [K]')\n",
    "plt.xlim((grid.min(), grid.max()))\n",
    "plt.ylabel(rf\"Desorption rate [$10^{{{y_fac}}}$ $m^{{{-2}}}$ $s^{{{-1}}}$]\")\n",
    "plt.ylim(bottom=0.)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, delete the campaign and dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.delete_campaign(campaign_id, verbose=True)\n",
    "tl.delete_dataset(dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinlab-client-um1gzrUq-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b4841770c9e34caa6ae18daa4abc2a1f1538c85b37a4e54ebb9908de806c0dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
